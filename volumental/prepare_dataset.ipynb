{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = pathlib.Path('/moby/datasets/interest_points_sts_concept')\n",
    "annotations_path = dataset_path / 'json'\n",
    "image_groups_path = dataset_path / 'large'\n",
    "image_paths = list(itertools.chain(*[p.iterdir() for p in image_groups_path.iterdir()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from volumental.moby.foot_detector.detector import TFLiteDetector\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "\n",
    "detector_name = 'demborg_object_detection_real_images_even_longer_1583411087'\n",
    "detector_file = '/moby/moby_foot_detector/experiments/{}/exported/detect.tflite'.format(detector_name)\n",
    "detections_dir = annotations_path / 'detections' / detector_name\n",
    "detections_dir.mkdir(exist_ok=True)\n",
    "\n",
    "images_to_run = filter(lambda p: not detection_path_from_image_path(p).exists(), image_paths)\n",
    "\n",
    "detector = None  # global variable to be initialized on each process\n",
    "def run_detector(image_path):\n",
    "    detection_path = detection_path_from_image_path(image_path)\n",
    "\n",
    "    if not detection_path.exists():\n",
    "        image = Image.open(img_path)\n",
    "        result = detector.coco_detect(image, 0, normalized_coordinates=True)\n",
    "        with open(detection_path, 'w') as detection_file:\n",
    "            json.dump({'detections': result}, detection_file) \n",
    "            \n",
    "def initialize():\n",
    "    global detector\n",
    "    detector = TFLiteDetector(detector_file, uniform_scaling=False, center_crop=True)\n",
    "    \n",
    "with multiprocessing.Pool(32, initializer=initialize) as pool:\n",
    "        list(tqdm.tqdm(pool.imap_unordered(run_detector, image_paths), total=len(image_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop and copy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pycocotools.mask as cocoUtils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = pathlib.Path('/moby/datasets/deepercut/volumental')\n",
    "cropped_path = dataset_path / 'cropped'\n",
    "cropped_keypoints_path = cropped_path / 'keypoints'\n",
    "cropped_path.mkdir(exist_ok=True)\n",
    "cropped_keypoints_path.mkdir(exist_ok=True)\n",
    "\n",
    "for image_group_path in image_groups_path.iterdir():\n",
    "    new_image_group_path = cropped_path / image_group_path.name\n",
    "    new_image_group_path.mkdir(exist_ok=True)\n",
    "\n",
    "def crop_image(image_path):\n",
    "    crop_image_path = cropped_path / image_path.parent.name / image_path.name\n",
    "    annotation_image_path = keypoint_path_from_cropped_image_path(image_path)\n",
    "    if crop_image_path.exists() and annotation_image_path.exists():\n",
    "        return\n",
    "    \n",
    "    annotation_path = annotation_path_from_image_path(image_path)\n",
    "    detection_path = detection_path_from_image_path(image_path)\n",
    "    try:\n",
    "        with open(annotation_path) as annotation_file:\n",
    "            annotations = json.load(annotation_file)\n",
    "            image_keypoints = annotations['image_keypoints']\n",
    "            bounding_box = annotations['bounding_box']\n",
    "        with open(detection_path) as detection_file:\n",
    "            detections = json.load(detection_file)\n",
    "            detected_boxes = [d['bbox'] for d in detections['detections']]\n",
    "            detected_box = select_best_detection(detected_boxes, bounding_box)\n",
    "            if detected_box is None:\n",
    "                #print('No box for', image_path)\n",
    "                return\n",
    "    except FileNotFoundError as e:\n",
    "        #print('Did not find file', e)\n",
    "        return\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    detected_box = np.array(detected_box)\n",
    "\n",
    "    padding = min(detected_box[0], detected_box[1]) * 0.3\n",
    "    half_padding = padding / 2.0\n",
    "\n",
    "    detected_box[:2] = detected_box[:2] - half_padding\n",
    "    detected_box[2:] = detected_box[2:] + padding\n",
    "\n",
    "    image_size = np.array(image.size)\n",
    "    detected_box = detected_box.reshape(2, 2)\n",
    "    detected_box *= image_size\n",
    "    detected_box = detected_box.reshape(4)\n",
    "    detected_box = (*detected_box[:2], *(detected_box[:2] + detected_box[2:]))\n",
    "\n",
    "    cropped_image = image.crop(detected_box)\n",
    "    cropped_image_size = cropped_image.size\n",
    "\n",
    "    cropped_image_keypoints = {}\n",
    "    num_visible = 0\n",
    "    for keypoint, (*coord, visible) in image_keypoints.items():\n",
    "        cropped_image_keypoints[keypoint] = [(coord[0] * image_size[0] - detected_box[0]), \n",
    "                                             (coord[1] * image_size[1] - detected_box[1])]\n",
    "        if visible:\n",
    "            num_visible += 1\n",
    "\n",
    "    if num_visible < 3:\n",
    "        return\n",
    "\n",
    "    cropped_image.save(crop_image_path)\n",
    "\n",
    "    with open(annotation_image_path, 'w') as annotation_path:\n",
    "        json.dump(cropped_image_keypoints, annotation_path)     \n",
    "\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #plt.figure()\n",
    "    #plt.imshow(cropped_image)\n",
    "    #print(cropped_image_keypoints)\n",
    "    #plt.scatter(cropped_image_keypoints['toe'][0], cropped_image_keypoints['toe'][1])\n",
    "    #break\n",
    "        \n",
    "with multiprocessing.Pool(32) as pool:\n",
    "    list(tqdm.tqdm(pool.imap_unordered(crop_image, image_paths), total=len(image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = [f'group_{i}' for i in range(7)]\n",
    "\n",
    "cropped_train_image_paths = list(itertools.chain(*[p.iterdir() for p in cropped_path.iterdir() if p.name in train_groups]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(next(cropped_keypoints_path.iterdir())) as keypoints_file:\n",
    "    keypoints = json.load(keypoints_file)\n",
    "\n",
    "keypoint_to_id = {keypoint: i for i, keypoint in enumerate(sorted(keypoints.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "dataset = []\n",
    "for image_path in tqdm.tqdm(cropped_train_image_paths):\n",
    "    keypoint_path = keypoint_path_from_cropped_image_path(image_path)\n",
    "    try:\n",
    "        with open(keypoint_path) as keypoint_file:\n",
    "            keypoints = json.load(keypoint_file)\n",
    "    except:\n",
    "        continue\n",
    "    keypoints = [[keypoint_to_id[keypoint], coords[0], coords[1]] for keypoint, coords in keypoints.items()]\n",
    "    entry = {\n",
    "        'image': str(image_path),\n",
    "        'size': [3, *image_size],\n",
    "        'joints': keypoints\n",
    "    }\n",
    "    #entry = [str(image_path), [3, *image_size], joints_dict]\n",
    "    dataset.append(entry)\n",
    "    #if i > 10:\n",
    "    #    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('dataset_python_train.mat', {'dataset': dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loaded = sio.loadmat('dataset.mat')['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
